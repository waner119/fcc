# 网页解析

## 1. 访问

### 1.1. 请求与响应

 `Requests` 基于 `urllib3` ，即多次请求重复使用一个 socket。

```python
import requests
# 所有的方法均是调用 requests(url) 完成
## requests.requests(url)

# GET
kv = {'wd': 'python'}
url = 'http://www.baidu.com/s'
r = requests.request('GET', url, params=kv)
print(r.url)
'http://www.baidu.com/s?wd=python'

# POST
r = requests.request('POST', url, params=kv)
## data
r = requests.request('POST', url, data=kv)
## json
r = requests.request('POST', url, json=kv)
## headers
hd = {'user-agent': 'Chrome/10'}
r = requests.request('POST', url, headers=hd)
## files
fs = {'file': open('data.xls', 'rb')}
r = requests.request('POST', url, files=fs)

# 响应
r = requests.get(url)
## r.text 返回的是 Unicode 型的数据
html = r.text
## 获得真实编码
r.apparent_encoding
## 设定编码
r.encoding='utf-8'

print(r.status_code)
print(r.text)
print(r.cookies)
# header 编码
print(r.encoding)
# 内容编码
print(r.apparent_encoding)
print(r.content)
```

### 1.2. 保存与编码

```python
import os

root = '/Downloads'
path = root + url.split('/')[-1]

try:
    if not os.path.exists(root):
        os.mkdir(root)
    if not os.path.exists(path):
        r = requests.get(url)
        ## r.text 为 str
        with open('test.html', 'w', encoding="utf-8") as f:
            f.write(r.text)
        ## r.content 为 bytes 型
        with open('test.html', 'wb') as f:
            r.encoding = r.apparent_encoding
        # r.content 为二进制形式
        f.write(r.content)
        f.close()
    else:
        print('file exists')
except:
    print('failure')
```

### 1.3. 异常与超时

```python
import requests
import time

def get_html(url):
    try:
        r = requests.get(url, timeout=30)
        # 返回异常
        r.raise_for_status()
        r.encoding = r.apparent_encoding
        return r.text
    except:
        return 'Error'

if __name__ == "__main__":
    url = "https://www.baidu.com"
    begin = time.time()
    get_html(url)
    end = time.time()

requests.get('http://github.com', timeout=0.001)
```

### 1.4. 重定向和流

```python
# 重定向
r = requests.head('http://github.com', allow_redirects=True)
r.url
'https://github.com/'
r.history
[<Response [301]>]

# 立即下载
tarball_url = 'https://github.com/kennethreitz/requests/tarball/master'
r = requests.get(tarball_url, stream=True)
```

### 1.5. 代理

若需要使用代理，可通过为任意请求方法提供 `proxies` 参数来配置单个请求:

```python
import requests

proxies = {"http": "http://10.10.1.10:3128", "https": "http://10.10.1.10:1080", }
requests.get("http://example.org", proxies=proxies)

# 也可通过环境变量 HTTP_PROXY 和 HTTPS_PROXY 来配置代理。
! export HTTP_PROXY="http://10.10.1.10:3128"
! export HTTPS_PROXY="http://10.10.1.10:1080"

requests.get("http://example.org")

# 若代理需要使用 HTTP Basic Auth，可使用 http://user:password@host/
proxies = {
  "http": "http://user:pass@10.10.1.10:3128/", 
}

# 要为某个特定的连接方式或者主机设置代理，使用 scheme://hostname 作为 key， 它会针对指定的主机和连接方式进行匹配。
proxies = {'http://10.20.1.128': 'http://10.10.1.10:5323'}

# 使用 SOCKS 代理和使用 HTTP 代理一样简单：
proxies = {'http': 'socks5://user:pass@host:port', 
'https': 'socks5://user:pass@host:port'}
```

### 1.6. 响应头链接字段

许多 HTTP API 都有响应头链接字段的特性，它们使得 API 能够更好地自我描述和自我显露。GitHub 在 API 中为 分页 [http://developer.github.com/v3/#pagination] 使用这些特性，例如：

```python
url = 'https://api.github.com/users/kennethreitz/repos?page=1&per_page=10'
r = requests.head(url=url)
r.headers['link']

# requests 会自动解析这些响应头链接字段，并使得它们非常易于使用:
r.links["next"]
{'url': 'https://api.github.com/users/kennethreitz/repos?page=2&per_page=10', 'rel': 'next'}
r.links["last"]
```

## 2. 会话

### 2.1. cookie

Cookie 是指某些网站服务器为了辨别用户身份和进行 `Session` 跟踪，而储存在用户浏览器上的文本文件，Cookie 可保持登录信息到用户下次与服务器的会话

```python
url = 'http://example.com/some/cookie/setting/url'
r = requests.get(url)
# 获取 cookie
r.cookies['example_cookie_name']
'example_cookie_value'

# 发送 cookies 到服务器：使用 cookies 参数：
url = 'http://httpbin.org/cookies'
cookies = dict(cookies_are='working')
r = requests.get(url, cookies=cookies)
r.text
'{"cookies": {"cookies_are": "working"}}'

# Cookie 的返回对象为 requestsCookieJar，它的行为和字典类似，但接口更为完整，适合跨域名跨路径使用
jar = requests.cookies.requestsCookieJar()
jar.set('tasty_cookie', 'yum', domain='httpbin.org', path='/cookies')
jar.set('gross_cookie', 'blech', domain='httpbin.org', path='/elsewhere'
url = 'http://httpbin.org/cookies'
r = requests.get(url, cookies=jar)
r.text
'{"cookies": {"tasty_cookie": "yum"}}'
```

### 2.2. 会话对象

会话对象让你能够跨请求保持某些参数。它也会在同一个 `Session` 实例发出的所有请求之间保持 cookie。所以若你向同一主机发送多个请求，底层的 TCP 连接将会被重用，从而带来显著的性能提升。 会话对象具有 requests API 的所有方法。

```python
s = requests.Session()
s.get('http://httpbin.org/cookies/set/sessioncookie/123456789')
r = s.get("http://httpbin.org/cookies")

print(r.text)
# '{"cookies": {"sessioncookie": "123456789"}}'

# 为请求方法提供缺省数据
s.auth = ('user', 'pass')
s.headers.update({'x-test': 'true'})
### both 'x-test' and 'x-test2' are sent
s.get('http://httpbin.org/headers', headers={'x-test2': 'true'})

# 任何你传递给请求方法的字典都会与已设置会话层数据合并。方法层的参数覆盖会话的参数。
r = s.get('http://httpbin.org/cookies', cookies={'from-my': 'browser'})
print(r.text)
### '{"cookies": {"from-my": "browser"}}'
r = s.get('http://httpbin.org/cookies')
print(r.text)
### '{"cookies": {}}'

# 用作前后文管理器
with requests.Session() as s:
  s.get('http://httpbin.org/cookies/set/sessioncookie/123456789')

# 准备请求
from requests import Request, Session

s = Session()
req = Request('GET', url, data=data, headers=header)
prepped = s.prepare_request(req)
# do something with prepped.body
# do something with prepped.headers
resp = s.send(prepped, stream=stream, verify=verify, proxies=proxies, 
cert=cert, timeout=timeout)
print(resp.status_code)
```

### 2.3. 证书

 `requests` 默认附带了一套它信任的根证书，来自于 Mozilla trust store
[https://hg.mozilla.org/mozilla-central/raw-file/tip/security/nss/lib/ckfw/builtins/certdata.txt]。然而它们在每次 requests 更新时才会更新。

```python
# 认证 SSL
requests.get('https://github.com', verify=True)
## verify = False，requests 可忽略对 SSL 证书的验证
requests.get('https://github.com', verify='/path/to/certfile')

# 本地证书用作客户端证书，可是单个文件（包含密钥和证书） 或一个包含两个文件路径的元组
requests.get('https://kennethreitz.org', cert = ('/path/client.cert', '/path/client.cert')
```

## 3. 身份认证

### 3.1. 基本认证

许多要求身份认证的 web 服务都接受 HTTP Basic Auth。这是最简单的一种身份认证，且 `requests` 对这种认证方式的支持是直接开箱即可用。以 HTTP Basic Auth 发送请求非常简单：

```python
from requests.auth import HTTPBasicAuth

requests.get('https://api.github.com/user', 
 auth=HTTPBasicAuth('user', 'pass'))

## <Response [200]>
```

### 3.2. 摘要式认证

另一种非常流行的 HTTP 身份认证形式是摘要式身份认证， `requests` 对它的支持亦为开箱即可用的：

```python
from requests.auth import HTTPDigestAuth

url = 'http://httpbin.org/digest-auth/auth/user/pass'
requests.get(url, auth=HTTPDigestAuth('user', 'pass'))
## <Response [200]>

from requests_oauthlib import OAuth1

url = 'https://api.twitter.com/1.1/account/verify_credentials.json'
auth = OAuth1('YOUR_APP_KEY', 'YOUR_APP_SECRET', ...
  'USER_OAUTH_TOKEN', 'USER_OAUTH_TOKEN_SECRET')
requests.get(url, auth=auth)
## <Response [200]>
```

### 3.3. 新身份认证

若你找不到所需要的身份认证形式的一个良好实现，你也可自己实现它。

 `requests` 非常易于添加你自己的身份认证形式。要想自己实现，就从 `AuthBase` 继承一个子类，并实现 `__call__()` 方法：

```python
import requests

class MyAuth(requests.auth.AuthBase):
  def __call__(self, r):
  # Implement my authentication
    return r

url = 'http://httpbin.org/get'
requests.get(url, auth=MyAuth())
## <Response [200]>
```

## 4. 数据交互

### 4.1. 分块编码请求

对于出去和进来的请求， `requests` 支持分块传输编码。要发送一个块编码的请求，仅需为你的请求体提供一个生成器（或任意没有具体长度的迭代器）。

```python
def gen():
	yield 'hi'
  yield 'there'

requests.post('http://some.url/chunked', data=gen())

# POST 多个分块编码的文件
## 你可在一个请求中发送多个文件。例如，设你要上传多个图像文件到一个 HTML 表单，使用一个多文件 field 叫做 “images”:
<input type="file" name="images" multiple="true" required="true"/>

## 要实现，只要把文件设到一个元组的列表中，其中元组结构为 (form_field_name, file_info)
url = 'http://httpbin.org/post'
multiple_files = [('images', ('foo.png', open('foo.png', 'rb'), 'image/png')), ('images', ('bar.png', open('bar.png', 'rb'), 'image/png'))]
r = requests.post(url, files=multiple_files)
r.text
{
  ...
  'files': {'images': 'data:image/png;base64, iVBORw ....'}
  'Content-Type': 'multipart/form-data; boundary=3131623adb2043caaeb5538cc7aa0b3a', 
  ...
}
```

### 4.2. 事件钩子

 `requests` 有一个钩子系统，你可用于操控部分请求过程，或信号事件处理。

```python
可用的钩子:
response:
  从一个请求产生的响应

# 可通过传递一个 {hook_name: callback_function} 字典给 hooks 请求参数为每个请求分配一个钩子函数：
hooks=dict(response=print_url)

# callback_function 会接受一个数据块作为它的第一个参数。
def print_url(r, *args, **kwargs):
  print(r.url)

# 若回调函数返回一个值，默认以该值替换传进来的数据。若函数未返回任何东西，也没有什么其他的影响。
# 在运行期间打印一些请求方法的参数：
requests.get('http://httpbin.org', hooks=dict(response=print_url))
## http://httpbin.org
## <Response [200]>
```

### 4.3. 流式请求

使用 `Response.iter_lines()` 可很方便地对流式 API （例如 Twitter 的流式 API[https://dev.twittercom/docs/streaming-api] ） 进行迭代。简单地设置 `stream` 为 `True` 便可使用 `iter_lines` 对相应进行迭代：

```python
import json
import requests
r = requests.get('http://httpbin.org/stream/20', stream=True)
for line in r.iter_lines():
  # filter out keep-alive new lines
  if line:decoded_line = line.decode('utf-8')
 print(json.loads(decoded_line))

# 当使用 decode_unicode=True 在 Response.iter_lines() 或 Response.iter_content() 中时，需要提供一个回退编码方式，以防服务器没有提供默认回退编码，从而导致错误：
if r.encoding is None:
  r.encoding = 'utf-8'
for line in r.iter_lines(decode_unicode=True):
  if line:
 print(json.loads(line))

# iter_lines 不保证重进入时的安全性。多次调用该方法 会导致部分收到的数据丢失。若你要在多处调用它，就应该使用生成的迭代器对象:
lines = r.iter_lines()
## 保存第一行以供后面使用，或者直接跳过
first_line = next(lines)
for line in lines:
  print(line)
```

## 5. 信息标记

### 5.1. 三大类型

| 标记形式 | 特点                         | 应用范围                     |
| -------- | ---------------------------- | ---------------------------- |
| XML      | 可扩展性好，但繁琐           | Internet 上的信息交互与传递  |
| JSON     | 信息有类型，适合程序处理     | 移动应用云端和节点的信息通信 |
| YAML     | 信息无类型，文本信息比例最高 | 各类系统的配置文件           |

### 5.2. XML

XML（eXtensible Markup Language）

![web_bs_xml](images/ch10/web_bs_xml.png)

```xml
<!-- 空元素的缩写形式 -->
<img src="china.jpg" size="10" />

<?xml version="1.0" encoding="ISO-8859-1"?>

<bookstore>（文档节点）

<book>
 <title lang="en"（属性节点）>Harry Potter</title>
 <author>J K. Rowling</author>（元素节点）
 <year>2005</year>
 <price>29.99</price>
</book>

</bookstore>
```

### 5.3. JSON

JSON（JavaScript Object Notation）为有类型的键值对

- 对象在 JS 中表示为 `{}` 中的内容
- 数组在 JS 中表示为 `[]` 中的内容

```json
"key" : ["value1", "value2"]
"key" : {"subkey" : "subvalue"}
```

### 5.4. YAML

YAML（YAML Ain’t Markup Language）

无类型键值对
- 缩进表达所属关系
- ‐ 表达并列关系
- | 表达整块数据

```yaml
# 注释
name:
  newName: 北京理工大学
  oldName: 延安自然科学院

name:
‐北京理工大学
‐延安自然科学院
```

## 6. 解析基础

### 6.1. 标签

| 基本元素        | Column B               |
| --------------- | ---------------------- |
| Tag             | 标签                   | 分别用<>和</>标明开头和结尾 |
| Name            | 标签名                 | <tag>.name                  |
| Attributes      | 字典形式组织           | <tag>.attrs                 |
| NavigableString | <>...</>中字符串       | <tag>.string                |
| Comment         | 标签内字符串的注释部分 |

```python
from bs4 import BeautifulSoup

# 解析
soup=BeautifulSoup('<html>data</html>', 'html.parser')
soup=BeautifulSoup('open('html_path')', 'html.parser')

# 美化数据
## lxml 的 html 解析器
soup=BeautifulSoup(html, 'lxml')
## lxml 的 xml 解析器
# soup=BeautifulSoup(html, 'xml')

print(soup.prettify())
print(soup.title)
print(soup.title.name)
print(soup.title.string)
print(soup.title.parent.name)
print(soup.p)
print(soup.p["class"])
print(soup.a)
```

### 6.2. 标签遍历

```python
下行遍历
## 子节点的列
print(soup.p.contents)
## 子节点的迭代类型
print(soup.p.children)
## 子孙节点的迭代类型
print(soup.p.descendants)
for child in soup.body.descendants:
  print(child)

# 上行遍历
print(soup.p.parent)
print(soup.p.parents)

# 平行遍历
print(soup.p.next_sibling)
print(soup.p.previous_sibling)
print(soup.p.next_siblings)
print(soup.p.previous_siblings)
```

### 6.3. 获取文本

 `get_text()` 会把正在处理的 HTML 文档中所有的标签都清除，然后返回一个只包含文字的字符串。假如正在处理一个包含许多超链接、段落和标签的大段源代码，则 `get_text()` 会把这些超链接、段落和标签都清除掉， 只剩下一串不带标签的文字。

通常在准备打印、存储和操作数据时，应该最后才使用 `get_text()` 。一般情况下，应该尽可能地保留 HTML 文档的标签结构。

```python
import requests
from bs4 import BeautifulSoup

url = "http://www.pythonscraping.com/pages/warandpeace.html"
r = requests.get(url)
html = r.text

bsObj = BeautifulSoup(html)
nameList = bsObj.find_all("span", {"class": "green"})
for name in nameList:
    print(name.get_text())
```

### 6.4. 查找

 `find()` 和 `find_all()` 可能是最常用的两个函数。借助它们，可通过标签的不同属性轻松地过滤 HTML 页面，查找需要的标签组或单个标签。

```python
find_all(tag, attributes, recursive, text, limit, keywords)
find(tag, attributes, recursive, text, keywords)
# find 等价于 find_all 的 limit = 1 时的情形
# 参数 limit 设置后，获得的前几项结果是按网页上的顺序排序的，未必是你想要的那前几项。

url = "http://www.pythonscraping.com/pages/warandpeace.html"
r = requests.get(url)
html = r.text

nameList = bsObj.find_all(text="the prince") print(len(nameList))

# 下面两行代码是完全一样的
bsObj.find_all(id="text")
bsObj.find_all("", {"id":"text"})
```

通过标签参数 tag 把标签列表传到 `find_all()` 里获取一列标签，其实就是一个"或"关系的过滤器。

若标签列表很长，就需要花很长时间。而关键词参数 keyword 可让你增加一个"与"关系的过滤器来简化工作。

用 keyword 偶尔会出现问题，尤其是在用 class 属性查找标签的时候， 因为 class 是 Python 中受保护的关键字。

```python
## 可用 BeautifulSoup 提供的有点儿臃肿的方案，在 class 后面增加一个下划线
bsObj.find_all(class_="green")
## 也可用属性参数把 class 用引号包起来
bsObj.find_all("", {"class":"green"})
```

### 6.5. 导航树

 `find_all()` 通过标签的名称和属性来查找标签 。但，若需要通过标签在文档中的位 置来查找标签，该怎么办？这就是导航树（Navigating Trees）的作用。看过用单一方向进行 BeautifulSoup 标签树的导航：

一般情况下，BeautifulSoup 函数总是处理当前标签的后代标签。例如， `bsObj.body.h1` 选择了 `body` 标签后代里的**第一个** `h1` 标签，不会去找 `body` 外面的标签。 类似地， `bsObj.div.find_all("img")` 会找出文档中**第一个** `div` 标签，然后获取这个 `div` 后 代里所有的 `img` 标签列表。

若选择 `bsObj.table.tr` 或直接用 `bsObj.tr` 来获取表格中的第一行，也可获得正确的结果。但，若想让爬虫更稳定，最好还是让标签的选择更加具体，这可避免各种意外。若有属性，就利用标签的属性。

```python
bsObj.find("table", {"id":"giftList"}).tr
```

## 7. 解析进阶

### 7.1. 定位器

```python
# Node
## / vs >
xpath = '/html/body/div'
css_locator = 'html > body > div'
## // vs blank space
xpath = '//div/span//p'
css_locator = ' div > span p'
## [N] vs :nth-of-type(N)
XPath: '//div/p[2]'
css_locator = 'div > p:nth-of-type(2)'

# Property
## [@id='uid'] vs by #uid
xpath = '//div[@id="uid"]/span//h4'
css_locator = 'div#uid > span h4'
## [@class='class'] vs .class
xpath = '//*[@class="class"]'
css_locator = ' .class'

# Attribute
## ./@href vs ::attr(href)
xpath = '//div[@id="uid"]/a/@href'
css_locator = 'div#uid > a::attr(href)'
```

### 7.2. 选择器

```python
# 标准选择器
print(soup.find_all('a'))
print(soup.find(id='link3'))
print(soup.find_all(attrs={'name': 'elements'}))

## 获取所有的链接，以及文字内容
for link in soup.find_all('a'):
    print(link.get('href'))

# CSS 选择器
print(soup.select('.panel .panel-heading'))
print(soup.select('ul li'))
print(soup.select('#list-2 .element'))

for li in soup.select('li'):
    print(li.get_text())

for ul in soup.select('ul'):
    print(ul['id'])
    print(ul.attrs['id'])
```
