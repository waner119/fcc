# 爬虫

## 1. 简介

根据使用场景，网络爬虫可分为通用爬虫和聚焦爬虫两种

- 通用爬虫: 捜索引擎抓取系统的重要组成部分，主要目的是将互联网上的网页下载到本地，形成一个互联网内容的镜像备份，不能爬取媒体文件
- 聚焦爬虫: 是"面向特定主题需求"的一种网络爬虫程序，在实施网页抓取时会对内容进行处理筛选，尽量保证只抓取与需求相关的网页信息

## 2. 静态采集

### 2.1. 遍历单个域名

仔细观察那些指向词条页面（不是指向其他内容页面）的链接，会发现它们都有三个共同点：

- 它们都在 `id=bodyContent` 的 `div` 标签里
- URL 链接不包含分号
- URL 链接都以 `/wiki/` 开头

运行代码，就会看到维基百科上 Kevin Bacon 词条里所有指向其他词条的链接。

```python
import requests
from bs4 import BeautifulSoup
import re

url = "http://en.wikipedia.org/wiki/Kevin_Bacon"
html = requests.get(url).text
bsObj = BeautifulSoup(html)
for link in bsObj.find("div", {
        "id": "bodyContent"
}).find_all("a", href=re.compile("^(/wiki/)((？!:).)*$")):
    if 'href' in link.attrs:
        print(link.attrs['href'])
```

当然，写程序来找出这个静态的维基百科词条里所有的词条链接很有趣，不过没什么实际 用处。我们需要让这段程序更像下面的形式。

- 一个函数 getLinks，可用维基百科词条 /wiki/<词条名称> 形式的 URL 链接作为参数， 然后以同样的形式返回一个列表，里面包含所有的词条 URL 链接。
- 一个主函数，以某个起始词条为参数调用 getLinks，再从返回的 URL 列表里随机选择 一个词条链接，再调用 getLinks，直到主动停止，或者在新的页面上没有词条链接 了，程序才停止运行。

```python
import requests
from bs4 import BeautifulSoup
import datetime
import random
import re

random.seed(datetime.datetime.now())

def getLinks(articleUrl):
    url = "http://en.wikipedia.org"+articleUrl
    html = requests.get(url).text
    bsObj = BeautifulSoup(html)
    return bsObj.find("div", {"id":"bodyContent"}).find_all("a", href=re.compile("^(/wiki/)((？!:).)*$"))

links = getLinks("/wiki/Kevin_Bacon")

while len(links) > 0:
    newArticle = links[random.randint(0, len(links)-1)].attrs["href"]
    print(newArticle)
    links = getLinks(newArticle)
```

### 2.2. 深网和暗网

深网（deep Web）是网络的一部分，与浅网（surface Web）对立。浅网是互联网上搜索引擎可抓到的那部分网络。据不完全统计，互联网中其实约 90% 的网络均是深网。因为谷歌不能做像表单提交这类事情，也找不到那些没有直接链接到顶层域名上的网页，或者因为有 robots.txt 禁止而不能查看网站。

暗网，也被称为 Darknet 或 dark Internet，完全是另一种"怪兽"。它们也建立在已有的网络基础上，但，使用 Tor 客户端，带有运行在 HTTP 之上的新协议，提供了一个信息交换的安全隧道。

和暗网不同，深网是相对容易采集的。实际上，很多工具均是在教你如何采集那些 Google 爬虫机器人不能获取的深网信息。

### 2.3. 采集整个网站

采集整个网站是一种非常耗费内存资源的过程，尤其是处理大型网站时，最合适的工具就是用一个数据库来储存采集的资源。

遍历整个网站的网络数据采集有许多好处。

- 生成网站地图
- 收集数据（需要爬虫有足够的深度）

为了避免一个页面被采集两次，链接去重是非常重要的。在代码运行时，把已发现的所有链接都放到一起，并保存在方便查询的 set 里。只有新链接才会被采集，之后再从页面中搜索其他链接

Python 默认的递归限制是 1000 次。因为维基百科的网络链接浩如烟海，所以这个程序达到递归限制后就会停止。

对于那些链接深度少于 1000 的"普通"网站，这个方法通常可正常运行， 一些奇怪的异常除外。例如，有一个在生成博文内链的规则。这个规则是"当前页面把 /blog/title_of_blog.php 加到它后面，作为本页面的 URL 链接"。 问题是它们可能会把 /blog/title_of_blog.php 加到一个已经有 /blog/ 的 URL 上 面了。故，网站就多了一个 /blog/。最后，爬虫找到了这样的 URL 链 接:/blog/blog/blog/blog.../blog/title_of_blog.php。 若不去检查这些问题，爬虫很快就会崩溃。

```python
import requests
from bs4 import BeautifulSoup
import re

pages = set()


def getLinks(pageUrl):
    global pages
    url = "http://en.wikipedia.org" + pageUrl
    html = requests.get(url).text
    bsObj = BeautifulSoup(html)
    for link in bsObj.find_all("a", href=re.compile("^(/wiki/)")):
        if 'href' in link.attrs:
            if link.attrs['href'] not in pages:
                # 遇到新页面
                newPage = link.attrs['href']


print(newPage)
pages.add(newPage)
getLinks(newPage)
getLinks("")
```

### 2.4. 收集整个网站数据

通过观察几个维基百科页面，包括词条和非词条页面，比如隐私策略之类的 页面，就会得出下面的规则。

- 所有的标题（所有页面上，不论是词条页面、编辑历史页面还是其他页面）均是在 `h1` → `span` 标签里，且，页面上只有一个 `h1` 标签。
- 所有的正文文字都在 `div#bodyContent` 标签里。但，若想进一步获取第一段文字，可能用 `div#mw-content-text` → `p` 更好（只选择第一段的标签）。这个规则对所有页面都适用，除了文件页面（例如，[wikipedia](https://en.wikipedia.org/wiki/File:Orbit_of_274301_Wikipedia.svg)），页面不包含内容文字（content text）的部分内容。
- 编辑链接只出现在词条页面上。若有编辑链接，都位于 `li#ca-edit` 标签的 `li#ca-edit` → `span` → `a` 里面。

调整前
 面的代码，我们就可建立一个爬虫和数据收集（至少是数据打印）的组合程序:

```python
import requests
from bs4 import BeautifulSoup

pages = set()

def getLinks(pageUrl):
    global pages
    url = "http://en.wikipedia.org"+articleUrl
    html = requests.get(url).text
    bsObj = BeautifulSoup(html)
    try:
    print(bsObj.h1.get_text())
        print(bsObj.find(id="mw-content-text").find_all("p")[0])
        print(bsObj.find(id="ca-edit").find("span").find("a").attrs['href'])

    except AttributeError:
        print("页面缺少一些属性!")

    for link in bsObj.find_all("a", href=re.compile("^(/wiki/)")):
        if 'href' in link.attrs:
            if link.attrs['href'] not in pages:
        # 我们遇到了新页面
            newPage = link.attrs['href']
            print("--------\n"+newPage)
            pages.add(newPage)
            getLinks(newPage)

getLinks("")
```

### 2.5. 通过互联网采集

就像之前的例子一样，后面要建立的网络爬虫亦为顺着链接从一个页面跳到另一个页面，描绘出一张网络地图。但，这一次，它们不再忽略外链，而是跟着外链跳转。

写爬虫随意跟随外链跳转之前，请问自己几个问题。

- 要收集哪些数据？这些数据可通过采集几个已经确定的网站（永远是最简单的做法）完成吗？爬虫需要发现可能不知道的网站吗？
- 当爬虫到了某个网站，它是立即顺着下一个出站链接跳到一个新网站，还是在网站上呆一会儿，深入采集网站的内容？
- 有没有我不想采集的一类网站？我对非英文网站的内容感兴趣吗？
- 若网络爬虫引起了某个网站网管的怀疑，如何避免法律责任？

网站首页上并不能保证一直能发现外链。这时为了能够发现外链，就需要递归地深入一个网站直到找到一个外链才停止。

```python
import requests
from bs4 import BeautifulSoup
import re
import datetime
import random

pages = set()
random.seed(datetime.datetime.now())

# 获取页面所有内链的列表
def getInternalLinks(bsObj, includeUrl):
    internalLinks = []
    # 找出所有以"/"开头的链接
    for link in bsObj.find_all("a"， href=re.compile("^(/|.*"+includeUrl+")")):
        if link.attrs['href'] is not None:
            if link.attrs['href'] not in internalLinks:
                internalLinks.append(link.attrs['href'])
    return internalLinks

# 收集网站上发现的所有外链列表
allExtLinks = set()
allIntLinks = set()
def getAllExternalLinks(siteUrl):
    html = requests.get(siteUrl).text
    bsObj = BeautifulSoup(html)
    internalLinks = getInternalLinks(bsObj, splitAddress(siteUrl)[0])
    externalLinks = getExternalLinks(bsObj, splitAddress(siteUrl)[0])
    for link in externalLinks:
        if link not in allExtLinks:
            allExtLinks.add(link)
            print(link)
    for link in internalLinks:
        if link not in allIntLinks:
            print("即将获取链接的 URL 是:"+link)
            allIntLinks.add(link)

getAllExternalLinks(link)

def splitAddress(address):
    addressParts = address.replace("http://", "").split("/")
    return addressParts

def getRandomExternalLink(startingPage):
    html = requests.get(startingPage).text
    bsObj = BeautifulSoup(html)
    externalLinks = getExternalLinks(bsObj, splitAddress(startingPage)[0])
    if len(externalLinks) == 0:
        internalLinks = getInternalLinks(startingPage)
        return getNextExternalLink(internalLinks[random.randint(0, len(internalLinks)-1)])
    else:
        return externalLinks[random.randint(0, len(externalLinks)-1)]

def followExternalOnly(startingSite):
    externalLink = getRandomExternalLink("http://oreilly.com")
    print("随机外链是:"+externalLink)

followExternalOnly(externalLink)
followExternalOnly("http://oreilly.com")
```

写代码之前拟个大纲或画个流程图是很好的编程习惯，这么做不仅可为后期处理节省很多时间，更重要的是可防止自己在爬虫变得越来越复杂时乱了分寸。（对 getAllExternalLinks()）

![web_internet](MD_Images/web_internet.png)

## 3. 微服务

### 3.1. 验证

很多新式 API 在使用之前都要求客户验证。有些 API 要求客户验证是为了计算 API 调用的费用，或者是提供了包月的服务。有些验证是为了"限制"用户使用 API（限制每秒钟、每小时或每天 API 调用的次数），或者是限制一部分用户对某种信息或某类 API 的访问。还有一些 API 可能不要求验证，但，可能会为了市场营销而跟踪用户的使用行为。

通常 API 验证的方法均是用类似令牌（token）的方式调用，每次 API 调用都会把令牌传递到服务器上。这种令牌要么是用户注册的时候分配给用户，要么就是在用户调用的时候才提供，可能是长期固定的值，也可能是频繁变化的，通过服务器对用户名和密码的组合处理后生成。

令牌除了在 URL 链接中传递，还会通过请求头里的 cookie 把用户信息传递给服务器。

```python
token = "<your api key>"
html = requests.post("http://myapi.com", headers={"token":token}).text
```

### 3.2. 处理 JSON

Python 把 JSON 转换成字典，JSON 数组转换成列表，JSON 字符串转换成 unicode 字符串。通过这种方式，可让 JSON 的获取和操作变得非常简单。

```python
import requests
import json

## 读取
def getCountry(ipAddress):
    url = "http://freegeoip.net/json/"+ipAddress
    html = requests.get(url).text
    html_json = json.loads(html)
    
    return html_json.get("country_code")

print(getCountry("50.78.253.58"))

# 将 Python 对象转换为 JSON 字符串
jsonString = '{"arrayOfNums": [{"number":0}, {"number":1}, {"number":2}], "arrayOfFruits": [{"fruit":"apple"}, {"fruit":"banana"}, {"fruit":"pear"}]}'

jsonObj = json.loads(jsonString)
print(jsonObj.get("arrayOfNums"))
[{'number': 0}, {'number': 1}, {'number': 2}]

print(jsonObj.get("arrayOfNums")[1])
{'number': 1}

print(jsonObj.get("arrayOfNums")[1].get("number")+
jsonObj.get("arrayOfNums")[2].get("number"))
3

print(jsonObj.get("arrayOfFruits")[2].get("fruit"))
pear
```

## 4. 动态采集

### 4.1. 常用 JavaScript 库

用 Python 执行 JavaScript 代码的效率非常低，尤其是在处理规模较大的 JavaScript 代码时。若有绕过 JavaScript 并直接解析它的方法（不需要执行它就可获得信息）会非常实用。

- jQuery

若在一个网站上看到了 jQuery，则采集这个网站数据的时候要格外小心。jQuery 可动态地创建 HTML 内容，只有在 JavaScript 代码执行之后才会显示。若用传统的方法采集页面内容，就只能获得 JavaScript 代码执行之前页面上的内容。

另外，这些页面还可能包含动画、用户交互内容和嵌入式媒体，这些内容对网络数据采集均是挑战。

- Google Analytics

有一半的网站都在用 Google Analytics，它可能是网站最常用的 JavaScript 库和最受欢迎的用户跟踪工具。

若网站使用了 Google Analytics，在页面底部会有类似如下所示的 JavaScript 代码：

```html
<!-- Google Analytics -->
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(["_setAccount", "UA-4591498-1"]);
  _gaq.push(["_setDomainName", "oreilly.com"]);
  _gaq.push(["_addIgnoredRef", "oreilly.com"]);
  _gaq.push(["_setSiteSpeedSampleRate", 50]);
  _gaq.push(["_trackPageview"]);

  (function () {
    var ga = document.createElement("script");
    ga.type = "text/javascript";
    ga.async = true;
    ga.src =
      ("https:" == document.location.protocol ? "https://ssl" : "http://www") +
      ".google-analytics.com/ga.js";
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(ga, s);
  })();
</script>
```

若一个网站使用了 Google Analytics 或其他类似的网络分析系统，而你不想让网站知道你在采集数据，就要确保把那些分析工具的 cookie 或者所有 cookie 都关掉。

## 5. Selenium

### 5.1. Selenium 选择器

Selenium 是一套完整的 web 应用程序测试系统，包含了测试的录制（selenium IDE），编写及运行（Selenium Remote Control）和测试的并行处理（Selenium Grid）。Selenium 的核心 Selenium Core 基于 JsUnit，完全由 JavaScript 编写，因此可用于任何支持 JavaScript 的浏览器上。

```python
# 声明浏览器对象
from selenium import webdriver
from selenium.webdriver.chrome.options import Options

chrome_options = Options()
# 无头模式启动
chrome_options.add_argument('--headless')
chrome_options.add_argument('--disable-gpu')
# 初始化实例
driver= webdriver.Chrome(chrome_options=chrome_options）
# 访问页面
driver.get("http://www.baidu.com")

## 单个元素查找
input_first = driver.find_element_by_id("q")
input_second = browser.find_element_by_css_selector("#q")
input_third = driver.find_element_by_xpath('//*[@id="q"]')
print(input_first)
print(input_second)
print(input_third)

## 多个元素查找
driver.get("http://www.taobao.com")
lis = driver.find_elements_by_css_selector('.service-bd li')
print(lis)

# 用 BeautifulSoup4 来解析网页内容，返回页面的源代码字符串
pageSource = driver.page_source
bsObj = BeautifulSoup(pageSource) print(bsObj.find(id="content").get_text()
```

### 5.2. 隐式等待

隐式等待（implicit wait）与显式等待的不同之处在于，隐式等待是等 DOM 中某个状态发生后再继续运行代码（没有明确的等待时间，但，有最大等待时限，只要在时限内就可），而显式等待明确设置了等待时间，如前面例子的等待三秒钟。在隐式等待中，DOM 触发的状态是用 expected_conditions 定义的。Selenium 里面元素被触发的期望条件（expected condition）有很多种，包括:

- 弹出一个提示框
- 一个元素被选中（比如文本框）
- 页面的标题改变了，或者某个文字显示在页面上或者某个元素里
- 一个元素在 DOM 中变成可见的，或者一个元素从 DOM 中消失了

当然，大多数的期望条件在使用前都需要先指定等待的目标元素。元素用定位器 （locator）指定。注意，定位器与选择器是不一样的。定位器是一种抽象的查询语言，用 By 对象表示，可用于不同的场合，包括创建选择器。

```python
from selenium.webdriver.support import expected_conditions as EC

# 定位器被用于查找 id = "loadedButton" 的按钮
EC.presence_of_element_located((By.ID, "loadedButton"))

# 创建选择器
selector = driver.find_element(By.ID, "content")
print(selector.text)
## 等价于
print(driver.find_element_by_id("content").text)
```

让 Selenium 不断地检查某个元素是否存在，以此确定页面是否已经完全加载，若页面加载成功就执行后面的程序。

```python
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

driver= webdriver.Chrome(chrome_options=chrome_options）driver.get("http://pythonscraping.com/pages/javascript/aJAXDemo.html")
try:
  element = WebDriverWait(driver, 10).until(
 EC.presence_of_element_located((By.ID, "loadedButton")))
finally:
  print(driver.find_element_by_id("content").text)
  driver.close()

# 元素交互
import time

input_str = browser.find_element_by_id('q')
input_str.send_keys("ipad")
time.sleep(1)
input_str.clear()
input_str.send_keys("MakBook pro")
button = browser.find_element_by_class_name('btn-search')
button.click()

# 交互动作
from selenium.webdriver import ActionChains

url = "http://www.runoob.com/try/try.php?filename=jqueryui-api-droppable"
browser.get(url)
browser.switch_to.frame('iframeResult')
source = browser.find_element_by_css_selector('#draggable')
target = browser.find_element_by_css_selector('#droppable')
actions = ActionChains(browser)
actions.drag_and_drop(source, target)
actions.perform()
```

### 5.3. 执行 JavaScript

```python
from selenium import webdriver

browser = webdriver.Chrome()
browser.get("http://www.zhihu.com/explore")

# 调用 js 方法
browser.execute_script('window.scrollTo(0, document.body.scrollHeight)')
browser.execute_script('alert("To Bottom")')

# 获取元素属性
logo = browser.find_element_by_id('zh-top-link-logo')
print(logo)
print(logo.get_attribute('class'))

# 获取文本值
input = browser.find_element_by_class_name('zu-top-add-question')
print(input.text)

# 获取 ID，位置，标签名
input = browser.find_element_by_class_name('zu-top-add-question')
print(input.id)
print(input.location)
print(input.tag_name)
print(input.size)
```

### 5.4. 客户端重定向

客户端重定向：在服务器将页面内容发送到浏览器前，由浏览器执行 JavaScript 完成的页面跳转，而不是服务器完成的跳转。当使用浏览器访问页面的时候，有时很难区分这两种重定向。由于客户端重定向执行很快，加载页面时你甚至感觉不到任何延迟，所以会让你觉得这个重定向就是一个服务器端重定向。

但，在进行网络数据采集的时候，这两种重定向的差异是非常明显的。服务器端重定向一般都可轻松地通过 Requests 解决，客户端重定向通过 Selenium 执行。这类重定向的主要问题是什么时候停止页面监控，即，怎么识别一个页面已经完成重定向。

我们可用一种智能的方法来检测客户端重定向是否完成，首先从页面开始加载时就"监视"DOM 中的一个元素，然后重复调用这个元素直到 Selenium 抛出一个 StaleElementReferenceException 异常；即，元素不在页面的 DOM 里了，说明这时网站已经跳转：

```python
from selenium import webdriver
import time
from selenium.webdriver.remote.webelement import WebElement
from selenium.common.exceptions import StaleElementReferenceException

def waitForLoad(driver):
    elem = driver.find_element_by_tag_name("html")
    count = 0
    while True:
        count += 1
        if count > 20:
            print("Timing out after 10 seconds and returning")
            return time.sleep(.5)
 try:
    elem == driver.find_element_by_tag_name("html")
 except StaleElementReferenceException:
     return driver = webdriver.Chrome(chrome_options=chrome_options)

driver.get("http://pythonscraping.com/pages/javascript/redirectDemo1.html")
waitForLoad(driver)
print(driver.page_source)
```

## 6. 单元测试

### 6.1. 填写表单并提交

```python
from selenium import webdriver
from selenium.webdriver.remote.webelement import WebElement from selenium.webdriver.common.keys import Keys
from selenium.webdriver import ActionChains

driver= webdriver.Chrome(chrome_options=chrome_options)
driver.get("http://en.wikipedia.org/wiki/Monty_Python")
driver.get("http://pythonscraping.com/pages/files/form.html")
firstnameField = driver.find_element_by_name("firstname")
lastnameField = driver.find_element_by_name("lastname")
submitButton = driver.find_element_by_id("submit")

### 方法 1
firstnameField.send_keys("Ryan") lastnameField.send_keys("Mitchell") submitButton.click()

### 方法 2
actions = ActionChains(driver).click(firstnameField)
 .send_keys("Ryan")
 .click(lastnameField)
 .send_keys("Mitchell")
 .send_keys(Keys.RETURN)
actions.perform()
print(driver.find_element_by_tag_name("body").text) driver.close()

print(driver.find_element_by_id("message").text)
element = driver.find_element_by_id("draggable")
target = driver.find_element_by_id("div2")
actions = ActionChains(driver)
actions.drag_and_drop(element, target).perform()
print(driver.find_element_by_id("message").text)
```

### 6.2. 鼠标拖放动作

```python
driver.get('http://pythonscraping.com/pages/javascript/draggableDemo.html')

print(driver.find_element_by_id("message").text)
element = driver.find_element_by_id("draggable")
target = driver.find_element_by_id("div2")
actions = ActionChains(driver)
actions.drag_and_drop(element, target).perform()
print(driver.find_element_by_id("message").text)
```

### 6.3. 截屏

```python
driver.get('http://www.pythonscraping.com/')

driver.get_screenshot_as_file('tmp/pythonscraping.png')
```
