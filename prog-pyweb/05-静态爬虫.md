# 静态爬虫

## 1. 简介

根据使用场景，网络爬虫可分为通用爬虫和聚焦爬虫两种

- 通用爬虫: 捜索引擎抓取系统的重要组成部分，主要目的是将互联网上的网页下载到本地，形成一个互联网内容的镜像备份，不能爬取媒体文件
- 聚焦爬虫: 是"面向特定主题需求"的一种网络爬虫程序，在实施网页抓取时会对内容进行处理筛选，尽量保证只抓取与需求相关的网页信息

## 2. 信息标记

### 2.1. 标记格式

| 标记格式 | 特点                        | 应用范围                     |
|----------|---------------------------|--------------------------|
| XML      | 可扩展性好，但繁琐           | Internet 上的信息交互与传递  |
| JSON     | 信息有类型，适合程序处理     | 移动应用云端和节点的信息通信 |
| YAML     | 信息无类型，文本信息比例最高 | 各类系统的配置文件           |

### 2.2. XML

XML（eXtensible Markup Language）

![web_bs_xml](./images/ch10/web_bs_xml.png)

```xml
<!-- 空元素的缩写形式 -->
<img src="china.jpg" size="10" />

<?xml version="1.0" encoding="ISO-8859-1"?>

<bookstore>（文档节点）

<book>
 <title lang="en"（属性节点）>Harry Potter</title>
 <author>J K. Rowling</author>（元素节点）
 <year>2005</year>
 <price>29.99</price>
</book>

</bookstore>
```

### 2.3. JSON

JSON（JavaScript Object Notation）为有类型的键值对

- 对象在 JS 中表示为 `{}` 中的内容
- 数组在 JS 中表示为 `[]` 中的内容

```json
"key" : ["value1", "value2"]
"key" : {"subkey" : "subvalue"}
```

### 2.4. YAML

YAML（YAML Ain’t Markup Language）

无类型键值对

- 缩进表达所属关系
- `‐` 表达并列关系
- `|` 表达整块数据

```yaml
# 注释
name:
  newName: 北京理工大学
  oldName: 延安自然科学院

name:
‐北京理工大学
‐延安自然科学院
```

## 3. 解析基础

### 3.1. 标签

| 基本元素        | Column B               |
| --------------- | ---------------------- |
| Tag             | 标签                   | 分别用<>和</>标明开头和结尾 |
| Name            | 标签名                 | <tag>.name                  |
| Attributes      | 字典形式组织           | <tag>.attrs                 |
| NavigableString | <>...</>中字符串       | <tag>.string                |
| Comment         | 标签内字符串的注释部分 |

```python
from bs4 import BeautifulSoup

# 解析
soup=BeautifulSoup('<html>data</html>', 'html.parser')
soup=BeautifulSoup('open('html_path')', 'html.parser')

# 美化数据
## lxml 的 html 解析器
soup=BeautifulSoup(html, 'lxml')
## lxml 的 xml 解析器
# soup=BeautifulSoup(html, 'xml')

print(soup.prettify())
print(soup.title)
print(soup.title.name)
print(soup.title.string)
print(soup.title.parent.name)
print(soup.p)
print(soup.p["class"])
print(soup.a)
```

### 3.2. 标签遍历

```python
下行遍历
## 子节点的列
print(soup.p.contents)
## 子节点的迭代类型
print(soup.p.children)
## 子孙节点的迭代类型
print(soup.p.descendants)
for child in soup.body.descendants:
  print(child)

# 上行遍历
print(soup.p.parent)
print(soup.p.parents)

# 平行遍历
print(soup.p.next_sibling)
print(soup.p.previous_sibling)
print(soup.p.next_siblings)
print(soup.p.previous_siblings)
```

### 3.3. 获取文本

 `get_text()` 会把正在处理的 HTML 文档中所有的标签都清除，然后返回一个只包含文字的字符串。假如正在处理一个包含许多超链接、段落和标签的大段源代码，则 `get_text()` 会把这些超链接、段落和标签都清除掉， 只剩下一串不带标签的文字。

通常在准备打印、存储和操作数据时，应该最后才使用 `get_text()` 。一般情况下，应该尽可能地保留 HTML 文档的标签结构。

```python
import requests
from bs4 import BeautifulSoup

url = "http://www.pythonscraping.com/pages/warandpeace.html"
r = requests.get(url)
html = r.text

bsObj = BeautifulSoup(html)
nameList = bsObj.find_all("span", {"class": "green"})
for name in nameList:
    print(name.get_text())
```

### 3.4. 查找

 `find()` 和 `find_all()` 可能是最常用的两个函数。借助它们，可通过标签的不同属性轻松地过滤 HTML 页面，查找需要的标签组或单个标签。

```python
find_all(tag, attributes, recursive, text, limit, keywords)
find(tag, attributes, recursive, text, keywords)
# find 等价于 find_all 的 limit = 1 时的情形
# 参数 limit 设置后，获得的前几项结果是按网页上的顺序排序的，未必是你想要的那前几项。

url = "http://www.pythonscraping.com/pages/warandpeace.html"
r = requests.get(url)
html = r.text

nameList = bsObj.find_all(text="the prince")
print(len(nameList))

# 下面两行代码是完全一样的
bsObj.find_all(id="text")
bsObj.find_all("", {"id":"text"})
```

通过标签参数 tag 把标签列表传到 `find_all()` 里获取一列标签，其实就是一个"或"关系的过滤器。

若标签列表很长，就需要花很长时间。而关键词参数 keyword 可让你增加一个"与"关系的过滤器来简化工作。

用 keyword 偶尔会出现问题，尤其是在用 class 属性查找标签的时候， 因为 class 是 Python 中受保护的关键字。

```python
## 可用 BeautifulSoup 提供的有点儿臃肿的方案，在 class 后面增加一个下划线
bsObj.find_all(class_="green")
## 也可用属性参数把 class 用引号包起来
bsObj.find_all("", {"class":"green"})
```

### 3.5. 导航树

 `find_all()` 通过标签的名称和属性来查找标签 。但，若需要通过标签在文档中的位 置来查找标签，该怎么办？这就是导航树（Navigating Trees）的作用。看过用单一方向进行 BeautifulSoup 标签树的导航：

一般情况下，BeautifulSoup 函数总是处理当前标签的后代标签。例如， `bsObj.body.h1` 选择了 `body` 标签后代里的**第一个** `h1` 标签，不会去找 `body` 外面的标签。 类似地， `bsObj.div.find_all("img")` 会找出文档中**第一个** `div` 标签，然后获取这个 `div` 后 代里所有的 `img` 标签列表。

若选择 `bsObj.table.tr` 或直接用 `bsObj.tr` 来获取表格中的第一行，也可获得正确的结果。但，若想让爬虫更稳定，最好还是让标签的选择更加具体，这可避免各种意外。若有属性，就利用标签的属性。

```python
bsObj.find("table", {"id":"giftList"}).tr
```

## 4. 选择器

### 4.1. 定位

```python
# Node
## / vs >
xpath = '/html/body/div'
css_locator = 'html > body > div'
## // vs blank space
xpath = '//div/span//p'
css_locator = ' div > span p'
## [N] vs :nth-of-type(N)
XPath: '//div/p[2]'
css_locator = 'div > p:nth-of-type(2)'

# Property
## [@id='uid'] vs by #uid
xpath = '//div[@id="uid"]/span//h4'
css_locator = 'div#uid > span h4'
## [@class='class'] vs .class
xpath = '//*[@class="class"]'
css_locator = ' .class'

# Attribute
## ./@href vs ::attr(href)
xpath = '//div[@id="uid"]/a/@href'
css_locator = 'div#uid > a::attr(href)'
```

### 4.2. 标签选择

```python
# 选择所有title标签
soup.select("title")
# 选择所有p标签中的第三个标签
soup.select("p:nth-of-type(3)") # 相当于soup.select(p)[2]
# 选择body标签下的所有a标签
soup.select("body a")
# 选择body标签下的直接a子标签
soup.select("body > a")
# 选择id=link1后的所有兄弟节点标签
soup.select("#link1 ~ .mysis")
# 选择id=link1后的下一个兄弟节点标签
soup.select("#link1 + .mysis")
```

### 4.3. 属性选择

```python
# 选择a标签，其属性中存在myname的所有标签
soup.select("a[myname]")
# 选择a标签，其属性href=http://example.com/lacie的所有标签
soup.select("a[href='http://example.com/lacie']")
# 选择a标签，其href属性以http开头
soup.select('a[href^="http"]')
# 选择a标签，其href属性以lacie结尾
soup.select('a[href$="lacie"]')
# 选择a标签，其href属性包含.com
soup.select('a[href*=".com"]')
# 从html中排除某标签，此时soup中不再有script标签
[s.extract() for s in soup('script')]
# 如果想排除多个呢
[s.extract() for s in soup(['script','fram']
```

## 5. 静态采集

### 5.1. 遍历单个域名

仔细观察那些指向词条页面（不是指向其他内容页面）的链接，会发现它们都有三个共同点：

- 它们都在 `id=bodyContent` 的 `div` 标签里
- URL 链接不包含分号
- URL 链接都以 `/wiki/` 开头

运行代码，就会看到维基百科上 Kevin Bacon 词条里所有指向其他词条的链接。

```python
import requests
from bs4 import BeautifulSoup
import re

url = "http://en.wikipedia.org/wiki/Kevin_Bacon"
html = requests.get(url).text
bsObj = BeautifulSoup(html)
for link in bsObj.find("div", {
        "id": "bodyContent"
}).find_all("a", href=re.compile("^(/wiki/)((？!:).)*$")):
    if 'href' in link.attrs:
        print(link.attrs['href'])
```

当然，写程序来找出这个静态的维基百科词条里所有的词条链接很有趣，不过没什么实际 用处。我们需要让这段程序更像下面的形式。

- 一个函数 getLinks，可用维基百科词条 /wiki/<词条名称> 形式的 URL 链接作为参数， 然后以同样的形式返回一个列表，里面包含所有的词条 URL 链接。
- 一个主函数，以某个起始词条为参数调用 getLinks，再从返回的 URL 列表里随机选择 一个词条链接，再调用 getLinks，直到主动停止，或者在新的页面上没有词条链接 了，程序才停止运行。

```python
import requests
from bs4 import BeautifulSoup
import datetime
import random
import re

random.seed(datetime.datetime.now())

def getLinks(articleUrl):
    url = "http://en.wikipedia.org"+articleUrl
    html = requests.get(url).text
    bsObj = BeautifulSoup(html)
    return bsObj.find("div", {"id":"bodyContent"}).find_all("a", href=re.compile("^(/wiki/)((？!:).)*$"))

links = getLinks("/wiki/Kevin_Bacon")

while len(links) > 0:
    newArticle = links[random.randint(0, len(links)-1)].attrs["href"]
    print(newArticle)
    links = getLinks(newArticle)
```

### 5.2. 深网和暗网

深网（deep Web）是网络的一部分，与浅网（surface Web）对立。浅网是互联网上搜索引擎可抓到的那部分网络。据不完全统计，互联网中其实约 90% 的网络均是深网。因为谷歌不能做像表单提交这类事情，也找不到那些没有直接链接到顶层域名上的网页，或者因为有 robots.txt 禁止而不能查看网站。

暗网，也被称为 Darknet 或 dark Internet，完全是另一种"怪兽"。它们也建立在已有的网络基础上，但，使用 Tor 客户端，带有运行在 HTTP 之上的新协议，提供了一个信息交换的安全隧道。

和暗网不同，深网是相对容易采集的。实际上，很多工具均是在教你如何采集那些 Google 爬虫机器人不能获取的深网信息。

### 5.3. 采集整个网站

采集整个网站是一种非常耗费内存资源的过程，尤其是处理大型网站时，最合适的工具就是用一个数据库来储存采集的资源。

遍历整个网站的网络数据采集有许多好处。

- 生成网站地图
- 收集数据（需要爬虫有足够的深度）

为了避免一个页面被采集两次，链接去重是非常重要的。在代码运行时，把已发现的所有链接都放到一起，并保存在方便查询的 set 里。只有新链接才会被采集，之后再从页面中搜索其他链接

Python 默认的递归限制是 1000 次。因为维基百科的网络链接浩如烟海，所以这个程序达到递归限制后就会停止。

对于那些链接深度少于 1000 的"普通"网站，这个方法通常可正常运行， 一些奇怪的异常除外。例如，有一个在生成博文内链的规则。这个规则是"当前页面把 /blog/title_of_blog.php 加到它后面，作为本页面的 URL 链接"。 问题是它们可能会把 /blog/title_of_blog.php 加到一个已经有 /blog/ 的 URL 上 面了。故，网站就多了一个 /blog/。最后，爬虫找到了这样的 URL 链 接:/blog/blog/blog/blog.../blog/title_of_blog.php。 若不去检查这些问题，爬虫很快就会崩溃。

```python
import requests
from bs4 import BeautifulSoup
import re

pages = set()


def getLinks(pageUrl):
    global pages
    url = "http://en.wikipedia.org" + pageUrl
    html = requests.get(url).text
    bsObj = BeautifulSoup(html)
    for link in bsObj.find_all("a", href=re.compile("^(/wiki/)")):
        if 'href' in link.attrs:
            if link.attrs['href'] not in pages:
                # 遇到新页面
                newPage = link.attrs['href']


print(newPage)
pages.add(newPage)
getLinks(newPage)
getLinks("")
```

### 5.4. 收集整个网站数据

通过观察几个维基百科页面，包括词条和非词条页面，比如隐私策略之类的 页面，就会得出下面的规则。

- 所有的标题（所有页面上，不论是词条页面、编辑历史页面还是其他页面）均是在 `h1` → `span` 标签里，且，页面上只有一个 `h1` 标签。
- 所有的正文文字都在 `div#bodyContent` 标签里。但，若想进一步获取第一段文字，可能用 `div#mw-content-text` → `p` 更好（只选择第一段的标签）。这个规则对所有页面都适用，除了文件页面（例如，[wikipedia](https://en.wikipedia.org/wiki/File:Orbit_of_274301_Wikipedia.svg)），页面不包含内容文字（content text）的部分内容。
- 编辑链接只出现在词条页面上。若有编辑链接，都位于 `li#ca-edit` 标签的 `li#ca-edit` → `span` → `a` 里面。

调整前
 面的代码，我们就可建立一个爬虫和数据收集（至少是数据打印）的组合程序:

```python
import requests
from bs4 import BeautifulSoup

pages = set()

def getLinks(pageUrl):
    global pages
    url = "http://en.wikipedia.org"+articleUrl
    html = requests.get(url).text
    bsObj = BeautifulSoup(html)
    try:
    print(bsObj.h1.get_text())
        print(bsObj.find(id="mw-content-text").find_all("p")[0])
        print(bsObj.find(id="ca-edit").find("span").find("a").attrs['href'])

    except AttributeError:
        print("页面缺少一些属性!")

    for link in bsObj.find_all("a", href=re.compile("^(/wiki/)")):
        if 'href' in link.attrs:
            if link.attrs['href'] not in pages:
        # 我们遇到了新页面
            newPage = link.attrs['href']
            print("--------\n"+newPage)
            pages.add(newPage)
            getLinks(newPage)

getLinks("")
```

### 5.5. 通过互联网采集

就像之前的例子一样，后面要建立的网络爬虫亦为顺着链接从一个页面跳到另一个页面，描绘出一张网络地图。但，这一次，它们不再忽略外链，而是跟着外链跳转。

写爬虫随意跟随外链跳转之前，请问自己几个问题。

- 要收集哪些数据？这些数据可通过采集几个已经确定的网站（永远是最简单的做法）完成吗？爬虫需要发现可能不知道的网站吗？
- 当爬虫到了某个网站，它是立即顺着下一个出站链接跳到一个新网站，还是在网站上呆一会儿，深入采集网站的内容？
- 有没有我不想采集的一类网站？我对非英文网站的内容感兴趣吗？
- 若网络爬虫引起了某个网站网管的怀疑，如何避免法律责任？

网站首页上并不能保证一直能发现外链。这时为了能够发现外链，就需要递归地深入一个网站直到找到一个外链才停止。

```python
import requests
from bs4 import BeautifulSoup
import re
import datetime
import random

pages = set()
random.seed(datetime.datetime.now())

# 获取页面所有内链的列表
def getInternalLinks(bsObj, includeUrl):
    internalLinks = []
    # 找出所有以"/"开头的链接
    for link in bsObj.find_all("a"， href=re.compile("^(/|.*"+includeUrl+")")):
        if link.attrs['href'] is not None:
            if link.attrs['href'] not in internalLinks:
                internalLinks.append(link.attrs['href'])
    return internalLinks

# 收集网站上发现的所有外链列表
allExtLinks = set()
allIntLinks = set()
def getAllExternalLinks(siteUrl):
    html = requests.get(siteUrl).text
    bsObj = BeautifulSoup(html)
    internalLinks = getInternalLinks(bsObj, splitAddress(siteUrl)[0])
    externalLinks = getExternalLinks(bsObj, splitAddress(siteUrl)[0])
    for link in externalLinks:
        if link not in allExtLinks:
            allExtLinks.add(link)
            print(link)
    for link in internalLinks:
        if link not in allIntLinks:
            print("即将获取链接的 URL 是:"+link)
            allIntLinks.add(link)

getAllExternalLinks(link)

def splitAddress(address):
    addressParts = address.replace("http://", "").split("/")
    return addressParts

def getRandomExternalLink(startingPage):
    html = requests.get(startingPage).text
    bsObj = BeautifulSoup(html)
    externalLinks = getExternalLinks(bsObj, splitAddress(startingPage)[0])
    if len(externalLinks) == 0:
        internalLinks = getInternalLinks(startingPage)
        return getNextExternalLink(internalLinks[random.randint(0, len(internalLinks)-1)])
    else:
        return externalLinks[random.randint(0, len(externalLinks)-1)]

def followExternalOnly(startingSite):
    externalLink = getRandomExternalLink("http://oreilly.com")
    print("随机外链是:"+externalLink)

followExternalOnly(externalLink)
followExternalOnly("http://oreilly.com")
```

写代码之前拟个大纲或画个流程图是很好的编程习惯，这么做不仅可为后期处理节省很多时间，更重要的是可防止自己在爬虫变得越来越复杂时乱了分寸。（对 getAllExternalLinks()）

![web_internet](./images/web_internet.png)
